{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c219437",
   "metadata": {},
   "source": [
    "# Chapter 12 - Custom Models and Training with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318b8fe",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tensorflow's Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9635611",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![tf_architecture](./images/ch12_tensorflows_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f01953",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Using TensorFlow like NumPy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a66b8eeb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-06 15:09:54.235583: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-06 15:09:54.235615: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab99cd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tensors and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059cd471",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1cc4e30",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "882540f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28f3fbc1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6ac81de",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 4.], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2f8bc6f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f043ca",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tensors and Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c5250f0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "195d6b9a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy() # or np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fde5a1ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9518e86",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de10ba",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice that NumPy uses 64-bit precision by default, while TensorFlow uses 32-bit. This is because 32-bit precision is generally more than enough for neural networks, plus it runs faster and uses less RAM. So when you create a tensor from a NumPy array, make sure to set `dtype=tf.float32`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414fee74",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Type Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8187c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Type conversions can significantly hurt performance, and they can easily go unnoticed when they are done automatically. To avoid this, TensorFlow does not perform any type conversions automatically: it just raises an exception if you try to execute an operation on tensors with incompatible types. For example, you cannot add a float tensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e20f42d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tf.constant(2.) + tf.constant(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9331c56b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tf.constant(2.) + tf.constant(40., dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7960b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To perform type conversion, use `tf.cast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c20c18e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t2 = tf.constant(40., dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6034e489",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf291ee",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110357bf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "tensorflow objects like `tf.constant` are immutable, meaning that we cannot use regular tensors to implement weights in a neural network. Our only option is to use `tf.Variable`, which works just like `tf.constant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6157851e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e641cbd",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13630/4069830452.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# constant\n",
    "t[0, 0] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49e26a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Customizing Models and Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842583bd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Custom Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc89d488",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose we want to implement a custom Huber Loss function (The Huber loss is not currently part of the official Keras API, but it is available in tf.keras just use an instance of the keras.losses.Huber class, but let's pretend it does not exist).\n",
    "\n",
    "We must simply create a function that receives the labels and the predictions as arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee26b6bb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use tensorflow operation to benefit from TensorFlow's graph features\n",
    "def huber_fn(y_true, y_pred):\n",
    "    \n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    \n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b7a2ef",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is also preferable to return a tensor containing one loss per instance, rather\n",
    "than returning the mean loss. This way, Keras can apply class weights or sample\n",
    "weights when requested (see Chapter 10).\n",
    "\n",
    "To use this custom loss function, just pass it to the loss argument in the `compile` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fbe003",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
    "model.fit(X_train, y_train, [...])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb2d51",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Saving and Loading Models That Contain Custom Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a92c8a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Once the model is saved, to load it with a custom loss function, for example, we need to provide a dictionary that maps the function name to the actual function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889aa75",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\", custom_objects={\"huber_fn\": huber_fn})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a079ff",
   "metadata": {
    "hidden": true
   },
   "source": [
    "note that the current implementation considers any error between -1 and 1 as small. If we wish to turn this threshold into a parameter, we need to create a function that outputs the custom loss function with the desired threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df737f0a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        \n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        \n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61789841",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22dd59c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "However, when saving the model, the threshold will not be saved. We need to pass it along with the function:\n",
    "\n",
    "```python\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\", custom_objects={\"huber_fn\": create_huber(2.0)})\n",
    "```\n",
    "\n",
    "*note that the name to use is `huber_fn`, which is the name of the function you gave Keras, not the name of the function that created it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6bd3f3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To solve this problem, we can create a subclass of the `keras.losses.loss` class, and then implementing its `get_config()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8f014be",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        \n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        \n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        \n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa2358",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The constructor accepts \\*\\*kwargs and passes them to the parent constructor, which handles standard hyperparameters: the name of the loss and the `reduction` algorithm to use to aggregate the individual instance losses. By default, it is `\"sum_over_batch_size\"`, which means that the loss will be the sum of the instance losses, weighted by the sample weights, if any, and divided by the batch size (not by the sum of weights, so this is not the weighted mean). Other possible values are `\"sum\"` and `None`.\n",
    "\n",
    "- The `call()` method takes the labels and predictions, computes all the instance losses, and returns them.\n",
    "\n",
    "- The `get_config()` method returns a dictionary mapping each hyperparameter name to its value. It first calls the parent class’s `get_config()` method, then adds the new hyperparameters to this dictionary (note that the convenient {\\*\\*x} syntax was added in Python 3.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886e442",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then, we can use any instance of this class when compiling the model. And when saving the model, the threshold will be saved along with it. To load the model, we just need to map the class name to the class itself:\n",
    "\n",
    "```python\n",
    "# compiling the model\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# loading the model\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\", custom_objects={\"HuberLoss\": HuberLoss})\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd89fa18",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Custom Activation Functions, Initializers, Regularizers, and Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ceb1d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Most Keras functionalities, such as losses, regularizers, constraints, initializers, metrics, activation functions, layers, and even full models, can be customized in very much the same way. Most of the time, you will just need to write a simple function with the appropriate inputs and outputs.\n",
    "\n",
    "Here are examples of a custom activation function (equivalent to `keras.activations.softplus()` or `tf.nn.softplus()`), a custom Glorot initializer (equivalent to `keras.initializers.glorot_normal()`), a custom $\\ell_1$ regularizer (equivalent to `keras.regularizers.l1(0.01)`), and a custom constraint that ensures weights are all positive (equivalent to `keras.constraints.nonneg()` or `tf.nn.relu()`):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60627a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b9d8df",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The arguments depend on the type of the custom function. These custom functions could be used as follows:\n",
    "\n",
    "```python\n",
    "layer = keras.layers.Dense(30, activation=my_softplus,\n",
    "                           kernel_initializer=my_glorot_initializer,\n",
    "                           kernel_regularizer=my_l1_regularizer,\n",
    "                           kernel_constraint=my_positive_weights)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb508eb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If a function has hyperparameters that need to be saved along with the model, then you must subclass the appropriate class, just as we did before. Examples of classes are `keras.regularizers.Regularizer`, `keras.constraints.Constraint`, `keras.initializers.Initializer`, or `keras.layers.Layer` (for any layer, including activation functions). Every class must have a `call()` method for losses, layers (including activation functions), and models, or the `__call__()` method for regularizers, initializers, and constraints. For metrics, things are a bit different, as we will see now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6938efc3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9d3bf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In most cases, defining a custom metric function is exactly the same as defining a custom loss function. In fact, we could even use the Huber loss function we created earlier as a metric. it would work just fine (and persistence\n",
    "would also work the same way, in this case only saving the name of the function, `\"huber_fn\"`):\n",
    "\n",
    "```python\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n",
    "```\n",
    "\n",
    "For each batch, during training, Keras will compute this metric and keep track of its mean since the beginning of the epoch. however, this is not always what we want. It makes no sense to average the precision of the classifier for each batch, it is simply wrong to compute it like this. What we need is to keep track of the true positives and false positives and that can compute their ratio when requested. This is what the `keras.metrics.Precision` class does:\n",
    "\n",
    "```python\n",
    ">>> precision = keras.metrics.Precision()\n",
    ">>> precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])\n",
    "<tf.Tensor: id=581729, shape=(), dtype=float32, numpy=0.8>\n",
    ">>> precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\n",
    "<tf.Tensor: id=581780, shape=(), dtype=float32, numpy=0.5>\n",
    "```\n",
    "\n",
    "Note that in this example, the precision metrics is used as a function that receives the labels and the predictions (could also receives sample weights). For each batch (function call), it updates the metrics (first 0.8, then 0.5). This is called a *streaming metric* (or *stateful metric*), as it is gradually updated, batch after batch. \n",
    "\n",
    "At any point, we can call the `result()` method to get the current value of the metric. See its variables with `variables` attribute, or use the `reset_states()` method to reset the value of the metric to 0.\n",
    "\n",
    "```python\n",
    ">>> p.result()\n",
    "<tf.Tensor: id=581794, shape=(), dtype=float32, numpy=0.5>\n",
    ">>> p.variables\n",
    "[<tf.Variable 'true_positives:0' [...] numpy=array([4.], dtype=float32)>,\n",
    "<tf.Variable 'false_positives:0' [...] numpy=array([4.], dtype=float32)>]\n",
    ">>> p.reset_states() # both variables get reset to 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c46d67",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To create a streaming metric, create a subclass of the `keras.metrics.Metric` class.\n",
    "\n",
    "```python\n",
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "```\n",
    "\n",
    "Walking through the code: \n",
    "\n",
    "- The constructor uses `add_weight()` to create the `tf.variables` need to keep track of the metric's state over multiple batches, but we could simply declare a `tf.Variable` that keras would keep track of it for us.\n",
    "\n",
    "- the `update_state()` method is called when we use an instance of this class as a function. It updates the variables for one batch. We are ignoring `sample_weight` for now.\n",
    "\n",
    "- the `result()` method returns the current state of the metric. When we use an instance of this class as a function, it calls the `update_state()`, then the `result()` state.\n",
    "\n",
    "- the default implementation of the `reset_states()` method resets all variables to 0.0 (but you can override it if needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc156c5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Custom Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4879f894",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To build a custom stateful layer (i.e., a layer with weights), we need to subclass the class `keras.layers.Layers`.\n",
    "\n",
    "```python\n",
    "\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    \n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "        \n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}\n",
    "```\n",
    "\n",
    "- The construct takes all default hyperparameters with `**kwargs`: this takes care of standard arguments such as `input_shape`, `trainable`, and `name`.\n",
    "\n",
    "- The `build()` method creates the layers' variables. We must pass the number of the neurons in the previous layer in order to create the connection weights matrix (i.e., the \"kernel\"). in the end we call the `super.build()` method to tell keras the layer was created. \n",
    "\n",
    "- The `call()` method performs the desired operations.\n",
    "\n",
    "- The `compute_output_shape()` method simply returns the shape of this layer’s outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d4c46d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Custom Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4a646",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We already saw in chapter 10 how to use the subclass API. As a plus, If you also want to be able to save the model using the `save()` method and load it using the `keras.models.load_model()` function, you must implement the `get_config()` method just like we did before.\n",
    "\n",
    "In addition, The `Model` class is a subclass of the `Layer` class, so models can be defined and\n",
    "used exactly like layers. But a model has some extra functionalities. Then why distinguish between the two? to be more explicitly and develop a cleaner code.\n",
    "\n",
    "With that, you can naturally and concisely build almost any model that you find in a paper, using the Sequential API, the Functional API, the Subclassing API, or even a mix of these. “Almost” any model? Yes, there are still a few things that we need to look at: first, how to define losses or metrics based on model internals, and second, how to build a custom training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e28046",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Losses and Metrics Based on Model Internals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b21e1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The custom losses and metrics we defined earlier were all based on the labels\n",
    "and the predictions (and optionally sample weights). There will be times when\n",
    "you want to define losses based on other parts of your model, such as the\n",
    "weights or activations of its hidden layers. This may be useful for regularization\n",
    "purposes or to monitor some internal aspect of your model.\n",
    "\n",
    "**To define a custom loss based on model internals, compute it based on any part\n",
    "of the model you want, then pass the result to the `add_loss()` method**.For\n",
    "example, let’s build a custom regression MLP model composed of a stack of five\n",
    "hidden layers plus an output layer. This custom model will also have an auxiliary\n",
    "output on top of the upper hidden layer. The loss associated to this auxiliary\n",
    "output will be called the *reconstruction loss* (see Chapter 17): it is the mean\n",
    "squared difference between the reconstruction and the inputs. By adding this\n",
    "reconstruction loss to the main loss, we will encourage the model to preserve as\n",
    "much information as possible through the hidden layers—even information that\n",
    "is not directly useful for the regression task itself. In practice, this loss\n",
    "sometimes improves generalization (it is a regularization loss). Here is the code\n",
    "for this custom model with a custom reconstruction loss:\n",
    "\n",
    "```python\n",
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "        kernel_initializer=\"lecun_normal\")\n",
    "        for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        return self.out(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eaabf6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The constructor creates a DNN with five hidden layers and one dense output layer.\n",
    "\n",
    "- The `build()` method creates an extra dense layer which will be used to reconstruct the inputs of the model. It must be created here because its number of units must be equal to the number of inputs, and this number is unknown before the `build()` method is called.\n",
    "\n",
    "- The `call()` method processes the inputs through all five hidden layers, then passes the result through the reconstruction layer, which produces the reconstruction.\n",
    "\n",
    "- Then the `call()` method computes the reconstruction loss (the mean squared difference between the reconstruction and the inputs), and adds it to the model’s list of losses using the `add_loss()` method.\n",
    "\n",
    "- Finally, the `call()` method passes the output of the hidden layers to the output layer and returns its output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950680e9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Computing Gradients Using Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce39437",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In chapter 10 (Appendix 10) we commented about *automatic differentiation* while introducing the tool that made possible training deep neural networks: the backpropagation algorithm. There are various autodiff techniques with its pros and cons. The one used by backpropagation is called reverse-mode autodiff.\n",
    "\n",
    "To understand how autodiff works, consider the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef77760",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f64a45",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From calculus we know that the gradient of f when $(w_1, w_2) = (5, 3)$ is $(36, 10)$. To get this solution, we can compute an approximation of each partial derivative by measuring how much the function's output changes when we tweak the corresponding parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaede538",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.000003007075065"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f93523d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000000003174137"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1, w2 + eps) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df946a2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The problem with this approach is that we'd have to call $f$ once per parameter. Instead, we should use autodiff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71551266",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "    \n",
    "gradients = tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7459ba58",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd7544b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Custom Training Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0b0ff",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sometimes we may need to implement our own training loop. Let's see how we can do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "665525c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00a3bfcd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "    kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec7fbedd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sample random batches\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6474f94",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print status bar\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                          for m in [loss] + (metrics or [])])\n",
    "    \n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "        \n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8feda33d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre/Documents/git/handson-ml-notes/handsonml2-env/lib/python3.8/site-packages/keras/optimizer_v2/nadam.py:73: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Nadam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters and choose the optimizer\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36905ef",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we are ready to build the custom loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1aa6c9b4",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11610/11610 - mean: 1.4295 - mean_absolute_error: 0.5820\n",
      "Epoch 2/5\n",
      "11610/11610 - mean: 0.6504 - mean_absolute_error: 0.5209\n",
      "Epoch 3/5\n",
      "11610/11610 - mean: 0.6789 - mean_absolute_error: 0.5335\n",
      "Epoch 4/5\n",
      "11610/11610 - mean: 0.6382 - mean_absolute_error: 0.5182\n",
      "Epoch 5/5\n",
      "11610/11610 - mean: 0.6535 - mean_absolute_error: 0.5260\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    \n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "            \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        mean_loss(loss)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "            \n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    \n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d7f986",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- First we create two training loops, one for epochs and the other for the batches.\n",
    "- Then, we sample a random batch from the training set\n",
    "- Inside the `tf.GradientTape()` block, we make a prediction for one batch (using the model as a function), and we compute the loss: it is equal to the main loss plus the other losses (in this model, there is one regularization loss per layer). Since the `mean_squared_error()` function returns one loss per instance, we compute the mean over the batch using `tf.reduce_mean()` (if you wanted to apply different weights to each instance, this is where you would do it). The regularization losses are already reduced to a single scalar each, so we just need to sum them (using `tf.add_n()`, which sums multiple tensors of the same shape and data type).\n",
    "- Next, we ask the tape to compute the gradient of the loss with regard to each trainable variable (not all variables!), and we apply them to the optimizer to perform a Gradient Descent step.\n",
    "- Then we update the mean loss and the metrics (over the current epoch), and we display the status bar.\n",
    "- At the end of each epoch, we display the status bar again to make it look complete and to print a line feed, and we reset the states of the mean loss and the metrics.\n",
    "\n",
    "If you set the optimizer’s `clipnorm` or `clipvalue` hyperparameter, it will take care of this for you. If you want to apply any other transformation to the gradients, simply do so before calling the `apply_gradients()` method.\n",
    "\n",
    "If you add weight constraints to your model (e.g., by setting `kernel_constraint` or `bias_constraint` when creating a layer), you should update the training loop to apply these constraints just after `apply_gradients()`:\n",
    "\n",
    "```python\n",
    "for variable in model.variables:\n",
    "    if variable.constraint is not None:\n",
    "        variable.assign(variable.constraint(variable))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd7bf9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9003ff",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. **How would you describe TensorFlow in a short sentence? What are its main features? Can you name other popular Deep Learning libraries?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06580ed6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "TensorFlow is an open-source library for numerical computation, particularly well suited and fine-tuned for large-scale Machine Learning. Its core is similar to NumPy, but it also features GPU support, support for distributed computing, computation graph analysis and optimization capabilities (with a portable graph format that allows you to train a TensorFlow model in one environment and run it in another), an optimization API based on reverse-mode autodiff, and several powerful APIs such as tf.keras, tf.data, tf.image, tf.signal, and more. Other popular Deep Learning libraries include PyTorch, MXNet, Microsoft Cognitive Toolkit, Theano, Caffe2, and Chainer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a886342",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2. **Is TensorFlow a drop-in replacement for NumPy? What are the main differences between the two?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fbf67e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "No. Even though TensorFlow offers similar operations, there are some significant changes to NumPy. The function names does not match, and some functions behave differently. Also, NumPy arrays are mutable, while TensorFlow tensors are not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274a1e16",
   "metadata": {
    "hidden": true
   },
   "source": [
    "3. **Do you get the same result with `tf.range(10)` and `tf.constant(np.arange(10))`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a3d7f93",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ff3094",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27d3546a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(np.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f5182",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The answer is no. What happens is that TensorFlow natively only deals with 32 bits objects. Meanwhile, NumPy natively creates objects of 64 bits. Therefore, the result may appear to be the same, but the data types are different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b716a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "4. **Can you name six other data structures available in TensorFlow, beyond regular tensors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a51f85",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- *Sparse Tensors* (`tf.SparseTensor`): Efficiently represent tensors containing mostly zeros.\n",
    "- *Tensor Arrays* (`tf.TensorArray`): Lists of tensors. \n",
    "- *Ragged Tensors* (`tf.RaggedTensor`): Represent static lists of lists of tensors, where every tensor has the same shape and data type.\n",
    "- *String Tensors*: regular tensors of type `tf.string`.\n",
    "- *Sets*: Represented as regular tensors (or sparse tensors)\n",
    "- *Queues*: Store tensors across multiple steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3336063d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "5. **A custom loss function can be defined by writing a function or by subclassing the `keras.losses.Loss` class. When would you use each option?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b08fe",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can simply write a function to represent a custom loss function when such function has no parameter to be persisted. If there is at least one parameter that must be persisted in order to make the code portable, then, we must use the subclassing API implementing the `get_config` method in the custom loss function class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd5589",
   "metadata": {
    "hidden": true
   },
   "source": [
    "6. **Similarly, a custom metric can be defined in a function or a subclass of `keras.metrics.Metric`. When would you use each option?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2148b8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we want to create a static metric which simply receives the inputs and outputs it, independently of any external contexts, we are safe to just implement it as a function. However, if our metrics need to be streamed during training, for instance, then we must implement it using the subclassing API to keep all states we are interested in. What I want to say is that if computing the metric over a whole epoch is not equivalent to computing the mean metric over all batches in that epoch, then we must subclass `keras.metrics.Metric`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c8c2e4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "7. **When should you create a custom layer versus a custom model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5493e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Custom models subclasses the `keras.models.Model`, which is a subclass of the `keras.layers.Layer` class. Therefore, a model can be viewed as a layer with a few more methods. However, for the sake of clarity of code, we must face these objects as different things, and implement a custom layers for layers and custom models for models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb88a5e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "8. **What are some use cases that require writing your own custom training loop?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d9590",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- You may want more control over the training process.\n",
    "- You may want to use more than one optimizer during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ac7b4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "9. **Can custom Keras components contain arbitrary Python code, or must they be convertible to TF Functions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29758eb6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "They can. However, this will kill the performance and optization operations performed by TensorFlow. The recommended is to stick to TF operations only inside the body of the function. If you absolutely\n",
    "need to include arbitrary Python code in a custom component, you can either wrap it in a `tf.py_function()` operation (but this will reduce performance and limit your model's portability) or set `dynamic=True` when creating the custom layer or model (or set `run_eagerly=True` when calling the model's `compile()` method)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa132ea",
   "metadata": {
    "hidden": true
   },
   "source": [
    "10. **What are the main rules to respect if you want a function to be convertible to a TF Function?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd78e3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Use TensorFlow operations whenever possible.\n",
    "- Do not use code with side effects such as logging or updating a counter because they will only run during tracing, which happens only in the first call to the function.\n",
    "- If there is no alternative other than use arbitrary python code in a function, wrap it with `ty.py_function()`.\n",
    "- Functions called inside other functions must be implemented over the sample principles.\n",
    "- If the function creates a TensorFlow variable, it must do so upon the very first call, and only then, or else you will get an exception.\n",
    "- The source code of the function must be available to TensorFlow.\n",
    "- TensorFlow will only capture `for` loops that iterate over a tensor or a dataset.\n",
    "- Prefer vectorized implementations whenever you can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94567956",
   "metadata": {
    "hidden": true
   },
   "source": [
    "11. **When would you need to create a dynamic Keras model? How do you do that? Why not make all your models dynamic?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79330c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Creating a dynamic Keras model can be useful for debugging, as it will not compile any custom component to a TF Function, and you can use any Python debugger to debug your code. It can also be useful if you want to include arbitrary Python code in your model (or in your training code), including calls to external libraries. To make a model dynamic, you must set `dynamic=True` when creating it. Alternatively, you can set `run_eagerly=True` when calling the model's `compile()` method. Making a model dynamic prevents Keras from using any of TensorFlow's graph features, so it will slow down training and inference, and you will not have the possibility to export the computation graph, which will limit your model's portability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc342b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Exercises **12** and **13** are not implemented, refer to [https://github.com/ageron/handson-ml2](https://github.com/ageron/handson-ml2)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
