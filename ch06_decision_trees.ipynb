{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cedfbd48",
   "metadata": {},
   "source": [
    "# Chapter 06 - Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08fe6e2",
   "metadata": {},
   "source": [
    "## Training and Visualizing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ed00f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edec143",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bdfbf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3845ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"decision_trees\"\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(PROJECT_ROOT_DIR, \"images\", fig_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "862c0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(tree_clf, out_file=image_path(\"ch06_iris_tree.dot\"),\n",
    "                feature_names=iris.feature_names[2:], class_names=iris.target_names,\n",
    "                rounded=True, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaf8d3e",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d6016cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: dot: can't open ch06_iris_tree.dot\r\n"
     ]
    }
   ],
   "source": [
    "!dot -Tpng ch06_iris_tree.dot -o ch06_iris_tree.png "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f572b",
   "metadata": {},
   "source": [
    "The decision tree classification process is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db0fdf4",
   "metadata": {},
   "source": [
    "![decision_tree](./images/iris_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c39439e",
   "metadata": {},
   "source": [
    "- the *samples* attribute counts how many training instances it applies to. For example, 100 training instances have a petal length greater than 2.45 cm (depth 1, right), and of those 100, 54 have a petal width smaller than 1.75 cm (depth 2, left).\n",
    "- the *value* attribute tells you how many training instances of each class this node applies to: for example, the bottom-right node applies to 0 Iris setosa, 1 Iris versicolor, and 45 Iris virginica.\n",
    "- Finally, a node’s *gini* attribute measures its impurity: a node is “pure” (gini=0) if all training instances it applies to belong to the same class. For example, since the depth-1 left node applies only to Iris setosa training instances, it is pure and its gini score is 0.\n",
    "\n",
    "Equation 6.1 show how the training algorithm computes the *gini* score $G_i$ of the $i^{th}$ node:\n",
    "\n",
    "$$G_i = 1 - \\sum_{k=1}^n p_{i, k}^2$$\n",
    "\n",
    "- $p_{i, k}$ is the ratio of class $k$ instances among the training instances in the $i^{th}$ node.\n",
    "\n",
    "*The depth-2 left node has a gini score equal to* $1 - (0/54)^2 - (49/54)^2 - (5/54)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30417bf9",
   "metadata": {},
   "source": [
    "## Estimating Class Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b3902c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['petal length (cm)', 'petal width (cm)']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features used to train the model\n",
    "iris[\"feature_names\"][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3dc1a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4 0.2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X[0])\n",
    "tree_clf.predict_proba([X[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9664d29",
   "metadata": {},
   "source": [
    "## The CART training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ad94d",
   "metadata": {},
   "source": [
    "Scikit-Learn uses the *Classification and Regression Tree* (CART) algorithm to train Decision Trees (also called “growing” trees). The algorithm works by first splitting the training set into two subsets using a single feature k and a threshold $t_k$ (e.g., “petal length ≤ 2.45 cm”). How does it choose k and $t_k$? It searches for the pair (k,  $t_k$) that produces the purest subsets (weighted by their size). Equation 6-2 gives the cost function that the algorithm tries to minimize.\n",
    "\n",
    "$$\n",
    "J(k, t_k) = \\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m} G_{right}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $G_{left/right}$ measures the impurity of the left/right subset\n",
    "- $m_{left/right}$ is the number of instance in the left/right subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408c6d4a",
   "metadata": {},
   "source": [
    "Once the CART algorithm has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the max_depth hyperparameter), or if it cannot find a split that will reduce impurity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a38dae",
   "metadata": {},
   "source": [
    "## Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e10dd",
   "metadata": {},
   "source": [
    "Making predictions requires traversing the Decision Tree from the root to a leaf. Decision Trees generally are approximately balanced, so traversing the Decision Tree requires going through roughly O(log (m)) nodes. Since each node only requires checking the value of one feature, the overall prediction complexity is O(log (m)), independent of the number of features. So predictions are very fast, even when dealing with large training sets.\n",
    "\n",
    "The training algorithm compares all features (or less if max_features is set) on all samples at each node. Comparing all features on all samples at each node results in a training complexity of O(n × m log (m)). For small training sets (less than a few thousand instances), Scikit-Learn can speed up training by presorting the data (set presort=True), but doing that slows down training considerably for larger training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847006bb",
   "metadata": {},
   "source": [
    "## Gini Impurity or Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457d449",
   "metadata": {},
   "source": [
    "By default, the Gini impurity measure is used, but you can select the entropy impurity measure instead by setting the *criterion* hyperparameter to \"entropy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217505d2",
   "metadata": {},
   "source": [
    "In Machine Learning, entropy is frequently used as an impurity measure: a set’s entropy is zero when it contains instances of only one class. Equation 6-3 shows the definition of the entropy of the $i^{th}$ node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35918dde",
   "metadata": {},
   "source": [
    "$$H_i = - \\sum_{k=1}^n p_{i, k}\\log_2(p_{i, k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991e593",
   "metadata": {},
   "source": [
    "So, should you use Gini impurity or entropy? The truth is, most of the time it does not make a big difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3968e24f",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439923f4",
   "metadata": {},
   "source": [
    "The DecisionTreeClassifier class has a few other parameters that similarly restrict the shape of the Decision Tree: *min_samples_split* (the minimum number of samples a node must have before it can be split), *min_samples_leaf* (the minimum number of samples a leaf node must have), *min_weight_fraction_leaf* (same as *min_samples_leaf* but expressed as a fraction of the total number of weighted instances), *max_leaf_nodes* (the maximum number of leaf nodes), and *max_features* (the maximum number of features that are evaluated for splitting at each node). Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1468f961",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3ab37",
   "metadata": {},
   "source": [
    "The CART algorithm works mostly the same way as earlier, except that instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the MSE. Equation 6-4 shows the cost function that the algorithm tries to minimize.\n",
    "\n",
    "$$\n",
    "J(k, t_k) = \\frac{m_{left}}{m} MSE_{left} + \\frac{m_{right}}{m} MSE_{right}\n",
    "$$\n",
    "\n",
    "\n",
    "where\n",
    "\n",
    "- $MSE_{node} = \\sum_{i \\in node} (\\hat{y}_{node} - y^{(i)})^2$\n",
    "- $\\hat{y}_{node} = \\frac{1}{m_{node}} \\sum_{i \\in node} y^{(i)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da2d1bf",
   "metadata": {},
   "source": [
    "## Instability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf5c0b",
   "metadata": {},
   "source": [
    "Hopefully by now you are convinced that Decision Trees have a lot going for\n",
    "them: they are simple to understand and interpret, easy to use, versatile, and\n",
    "powerful. However, they do have a few limitations. First, as you may have\n",
    "noticed, Decision Trees love orthogonal decision boundaries (all splits are\n",
    "perpendicular to an axis), which makes them sensitive to **training set rotation**.\n",
    "\n",
    "More generally, the main issue with Decision Trees is that **they are very sensitive\n",
    "to small variations in the training data**. Actually, since the training algorithm used by Scikit-Learn is stochastic, you may get very different models even on the same training data (unless you set the *random_state* hyperparameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac33daf",
   "metadata": {},
   "source": [
    "Random Forests can limit this instability by averaging predictions over many trees, as we will see in the next chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
