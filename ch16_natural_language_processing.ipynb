{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cd6ed3",
   "metadata": {},
   "source": [
    "# Chapter 16 - Natural Language Processing with RNNs and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377e557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 11:44:04.739374: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-26 11:44:04.739426: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd24dfd",
   "metadata": {},
   "source": [
    "## Generating Shakesperian Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165267f2",
   "metadata": {},
   "source": [
    "Let's build a Char-RNN, an RNN whose task is to predict the next character in a sentence. We'll train the RNN using some sentences from Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0977ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\" # shortcut url\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bfb6e",
   "metadata": {},
   "source": [
    "Next, we must encode every character as an integer. One option is to create a custom preprocessing layer, as we did in Chapter 13. But in this case, it will be simpler to use Keras’s `Tokenizer` class. First we need to fit a tokenizer to the text: it will find all the characters used in the text and map each of them to a different character ID, from 1 to the number of distinct characters (it does not start at 0, so we can use that value for masking, as we will see later in this chapter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d9952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079ed163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f424a378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f010e43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of distinct characters\n",
    "max_id = len(tokenizer.word_index)\n",
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbacbea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of characters\n",
    "dataset_size = tokenizer.document_count\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b462b",
   "metadata": {},
   "source": [
    "Let's encode the full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than from 1 to 39):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a879abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc2bf3",
   "metadata": {},
   "source": [
    "To split the dataset into a training set, a validation set, and a test set, we can't simply shuffle it, since we are dealing with sequential data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b14cb",
   "metadata": {},
   "source": [
    "### How to Split a Sequential Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a6951d",
   "metadata": {},
   "source": [
    "It is very important to avoid any overlap between the training set, the validation set, and the test set. For example, we can take the first 90% of the text for the training set, then the next 5% for the validation set, and the final 5% for the test set. It would also be a good idea to leave a gap between these sets to avoid the risk of a paragraph overlapping over two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b3f80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 11:17:46.311854: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-24 11:17:46.311888: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-24 11:17:46.311921: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (alexandre-Inspiron-5458): /proc/driver/nvidia/version does not exist\n",
      "2022-05-24 11:17:46.530533: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5803cafd",
   "metadata": {},
   "source": [
    "### Chopping the Sequential Dataset Into Multiple Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93d2dd",
   "metadata": {},
   "source": [
    "The training set now consists of a single sequence of over a million characters, so we can’t just train the neural network directly on it: the RNN would be equivalent to a deep net with over a million layers, and we would have a single (very long) instance to train it. Instead, we will use the dataset's `window()` method to convert this long sequence of characters into many smaller windows of text. Every instance in the dataset will be a fairly short substring of the whole text, and the RNN will be unrolled only over the length of these substrings. This is called *truncated backpropagation through time*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a37a4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1  # target = input shifted 1 character ahead\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ad8b9",
   "metadata": {},
   "source": [
    "The `window()` function returns a `nested dataset`, analogous to a list of lists. However, the `fit` method only accepts tensors as input. Therefore, we must use the function method `flat_map` to flatten that list of datasets. Note that we can also pass a function to `flat_map` to perform transform operations before flattening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "140e2701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since all windows have exactly that length, we will get a single tensor for each of them\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e678946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af880e23",
   "metadata": {},
   "source": [
    "In summary, so far we have performed the following operations:\n",
    "\n",
    "![pipeline_data_transformation](./images/ch16_pipeline_sequential_data_transformation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfb6a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the characters\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "# prefetching\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06177d",
   "metadata": {},
   "source": [
    "### Building and Training the Char-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0326d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, \n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7f730",
   "metadata": {},
   "source": [
    "### Using the Char-RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540f7d3",
   "metadata": {},
   "source": [
    "To feed new data to the model, we need to preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2fb7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6922ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]  # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d70c3",
   "metadata": {},
   "source": [
    "### Generating Fake Shakesperian Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433d938d",
   "metadata": {},
   "source": [
    "To generate new text using the Char-RNN model, we could feed it some text,\n",
    "make the model predict the most likely next letter, add it at the end of the text,\n",
    "then give the extended text to the model to guess the next letter, and so on. But\n",
    "in practice this often leads to the same words being repeated over and over\n",
    "again. Instead, we can pick the next character randomly, with a probability equal\n",
    "to the estimated probability, using TensorFlow’s `tf.random.categorical()`\n",
    "function. This will generate more diverse and interesting text. The\n",
    "`categorical()` function samples random class indices, given the class log\n",
    "probabilities (logits). To have more control over the diversity of the generated\n",
    "text, we can divide the logits by a number called the `temperature`, which we can\n",
    "tweak as we wish: a temperature close to 0 will favor the high-probability\n",
    "characters, while a very high temperature will give all characters an equal\n",
    "probability. The following `next_char()` function uses this approach to pick the\n",
    "next character to add to the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "841d654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess(text)\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944ffaeb",
   "metadata": {},
   "source": [
    "Next, we can write a small function that will repeatedly call `next_char()` to get\n",
    "the next character and append it to the given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2a7b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa2d2ab",
   "metadata": {},
   "source": [
    "Let's text some inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96b612cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c72d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(complete_text(\"w\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "606cf704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(complete_text(\"w\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042d58c",
   "metadata": {},
   "source": [
    "### Stateful RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07eff8b",
   "metadata": {},
   "source": [
    "Until now, we have used only *stateless* RNNs: at each training iteration the\n",
    "model starts with a hidden state full of zeros, then it updates this state at each\n",
    "time step, and after the last time step, it throws it away, as it is not needed\n",
    "anymore. What if we told the RNN to preserve this final state after processing\n",
    "one training batch and use it as the initial state for the next training batch? This\n",
    "way the model can learn long-term patterns despite only backpropagating\n",
    "through short sequences. This is called a *stateful* RNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9dbc4",
   "metadata": {},
   "source": [
    "First, note that a stateful RNN only makes sense if each input sequence in a\n",
    "batch starts exactly where the corresponding sequence in the previous batch left\n",
    "off. So the first thing we need to do to build a stateful RNN is to use sequential\n",
    "and nonoverlapping input sequences (rather than the shuffled and overlapping\n",
    "sequences we used to train stateless RNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6d60c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44caef14",
   "metadata": {},
   "source": [
    "![pipeline_stateful_rnn](./images/ch16_pipeline_stateful_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da7bc4",
   "metadata": {},
   "source": [
    "Now let's create the stateful RNN:\n",
    "\n",
    "- Set `stateful=True`\n",
    "- The RNN needs to know the batch size (since it will preserve a state for each input sequence in the batch), so we must set `batch_input_shape` in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8842dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True, \n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24a0ce",
   "metadata": {},
   "source": [
    "We need to reset the states at the end of each epoch, to do this, we can use the following callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3e8b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef30d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239871f1",
   "metadata": {},
   "source": [
    "After this model is trained, it will only be possible to use it to make predictions for batches of\n",
    "the same size as were used during training. To avoid this restriction, create an identical\n",
    "stateless model, and copy the stateful model’s weights to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90feebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tdf3zwvcik :ma!&q. :phgr&;ubltcpzhp:'rv:cq3z!$ pau:\n"
     ]
    }
   ],
   "source": [
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "# Build the model\n",
    "stateless_model.build(tf.TensorShape([None, None, max_id]))\n",
    "stateless_model.set_weights(model.get_weights())\n",
    "model = stateless_model\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0035442",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7990bfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 7s 0us/step\n",
      "17473536/17464789 [==============================] - 7s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16e954",
   "metadata": {},
   "source": [
    "The reviews have already been preprocessed for us: each of which is\n",
    "represented as a NumPy array of integers, where each integer represents a word.\n",
    "All punctuation was removed, and then words were converted to lowercase, split\n",
    "by spaces, and finally indexed by frequency (so low integers correspond to\n",
    "frequent words). The integers 0, 1, and 2 are special: they represent the padding\n",
    "token, the start-of-sequence (SSS) token, and unknown words, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8eb07932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 1s 1us/step\n",
      "1654784/1641221 [==============================] - 1s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode a review\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4aada",
   "metadata": {},
   "source": [
    "If we want to deploy the model to a mobile device or a web application, we may have to handle preprocessing only using TensorFlow operations, so it can be included in the model itself. Let's see how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fd5b535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the original IMDb dataset \n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "113545b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'unsupervised'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eab3ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = info.splits[\"train\"].num_examples\n",
    "test_size = info.splits[\"test\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66d243be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5641768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d3f86",
   "metadata": {},
   "source": [
    "Next, we need to construct the vocabulary. This requires going through the\n",
    "whole training set once, applying our `preprocess()` function, and using a\n",
    "Counter to count the number of occurrences of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8468fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e5a263a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66018012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53893"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea955c66",
   "metadata": {},
   "source": [
    "We probably don't need out model to know all the words in the dictionary, so let's keep only the 10000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89dfcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6edca0",
   "metadata": {},
   "source": [
    "Now we need to replace each word with its ID (i.e., its index in the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e02d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0679dc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cbac77",
   "metadata": {},
   "source": [
    "Now we are ready to create the final training set. We batch the reviews, then\n",
    "convert them to short sequences of words using the `preprocess()` function,\n",
    "then encode these words using a `simple encode_words()` function that uses the\n",
    "table we just built, and finally prefetch the next batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b315bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21ada926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 128s 159ms/step - loss: 0.6151 - accuracy: 0.6264\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 102s 131ms/step - loss: 0.3842 - accuracy: 0.8332\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 113s 144ms/step - loss: 0.2245 - accuracy: 0.9171\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 109s 140ms/step - loss: 0.1448 - accuracy: 0.9493\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 123s 158ms/step - loss: 0.1215 - accuracy: 0.9579\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f8f8f",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74754298",
   "metadata": {},
   "source": [
    "As it stands, the model will need to learn that the padding tokens should be\n",
    "ignored. But we already know that! Why don’t we tell the model to ignore the\n",
    "padding tokens, so that it can focus on the data that actually matters? It’s actually\n",
    "trivial: simply add `mask_zero=True` when creating the Embedding layer. This\n",
    "means that padding tokens (whose ID is 0) will be ignored by all downstream\n",
    "layers. That’s all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d129d7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 130s 156ms/step - loss: 0.5389 - accuracy: 0.7154\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 123s 157ms/step - loss: 0.3348 - accuracy: 0.8600\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 0.1849 - accuracy: 0.9337\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 133s 170ms/step - loss: 0.1298 - accuracy: 0.9551\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 125s 160ms/step - loss: 0.1178 - accuracy: 0.9560\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b39e3",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a283342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 12:22:26.846339: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 192762400 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ea9d34",
   "metadata": {},
   "source": [
    "The `hub.KerasLayer` layer downloads the module from the given URL. This\n",
    "particular module is a sentence encoder: it takes strings as input and encodes\n",
    "each one as a single vector (in this case, a 50-dimensional vector). Internally, it\n",
    "parses the string (splitting words on spaces) and embeds each word using an\n",
    "embedding matrix that was pretrained on a huge corpus: the Google News 7B\n",
    "corpus (seven billion words long!). Then it computes the mean of all the word\n",
    "embeddings, and the result is the sentence embedding. We can then add two\n",
    "simple Dense layers to create a good sentiment analysis model. By default, a\n",
    "hub.KerasLayer is not trainable, but you can set `trainable=True` when\n",
    "creating it to change that so that you can fine-tune it for your task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17c46fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 6s 6ms/step - loss: 0.5473 - accuracy: 0.7266\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5143 - accuracy: 0.7484\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5092 - accuracy: 0.7510\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5058 - accuracy: 0.7538\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5031 - accuracy: 0.7548\n"
     ]
    }
   ],
   "source": [
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets[\"train\"].batch(batch_size).prefetch(1)\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d282a304",
   "metadata": {},
   "source": [
    "## An Encoder-Decoder Network for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4296cd",
   "metadata": {},
   "source": [
    "Below, the architecture of a neural machine translation model is shown. The model translates english sentences to french.\n",
    "\n",
    "Note that the French translations are also used as inputs\n",
    "to the decoder, but shifted back by one step. In other words, the decoder is given\n",
    "as input the word that it should have output at the previous step (regardless of\n",
    "what it actually output). For the very first word, it is given the start-of-sequence\n",
    "(SOS) token. The decoder is expected to end the sentence with an end-ofsequence (EOS) token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dcf1c4",
   "metadata": {},
   "source": [
    "![simple_nmt](./images/ch16_simple_nmt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbe5eb",
   "metadata": {},
   "source": [
    "Note that at inference time (after training), you will not have the target sentence\n",
    "to feed to the decoder. Instead, simply feed the decoder the word that it output at\n",
    "the previous step, as shown in Figure 16-4 (this will require an embedding\n",
    "lookup that is not shown in the diagram)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1f6bb",
   "metadata": {},
   "source": [
    "![nmt_inference_time](./images/ch16_nmt_inference_time.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state, \n",
    "    sequence_length=sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8251c",
   "metadata": {},
   "source": [
    "The *TrainingSampler* is one of several samplers available in TensorFlow Addons:\n",
    "their role is to tell the decoder at each step what it should pretend the previous\n",
    "output was. During inference, this should be the embedding of the token that was\n",
    "actually output. During training, it should be the embedding of the previous\n",
    "target token: this is why we used the *TrainingSampler*. In practice, it is often a\n",
    "good idea to start training with the embedding of the target of the previous time\n",
    "step and gradually transition to using the embedding of the actual token that was\n",
    "output at the previous step. This idea was introduced in a 2015 paper by Samy\n",
    "Bengio et al. The *ScheduledEmbeddingTrainingSampler* will randomly\n",
    "choose between the target or the actual output, with a probability that you can\n",
    "gradually change during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ef41cb",
   "metadata": {},
   "source": [
    "It is worth noticing other strategies for improving NMTs: **Bidirectional RNNs**, and **Beam Search**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bed911",
   "metadata": {},
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a511254f",
   "metadata": {},
   "source": [
    "Consider the path from the word \"milk\" to its translation \"lait\" in Figure 16-3: it\n",
    "is quite long! This means that a representation of this word (along with all the\n",
    "other words) needs to be carried over many steps before it is actually used. Can’t\n",
    "we make this path shorter? This is the core idea behind attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7ceb8",
   "metadata": {},
   "source": [
    "The image below shows the simplified architecture of an encoder-decoder NMT with additive attention mechanism.\n",
    "\n",
    "The main differences are:\n",
    "\n",
    "- Instead of just sending the encoder’s final hidden state to the decoder (which is still done, although it is not shown in the figure), we now send all of its outputs to the decoder;\n",
    "- At each time step, the decoder's memory cell computes a weighted sum of all these encoder outputs: this determines which words it will focus on at this step. The weight $\\alpha_{(t, i)}$ is the weight of the $i^{th}$ encoder output at the $t^{th}$ decoder time step. For example, if the weight $\\alpha_{(3, 2)}$ is much larger than the weights $\\alpha_{(3, 0)}$ and $\\alpha_{(3, 0)}$, then the decoder will pay much more attention to word number 2 (\"milk\") than to the other two words, at least at this time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9812ba7a",
   "metadata": {},
   "source": [
    "![attention_mechanisms](./images/ch16_attention_mechanisms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce0acc",
   "metadata": {},
   "source": [
    "These $\\alpha_{(t, i)}$ are generated by a type of small NN called *allignment model* (or an attention layer), which is trained jointly with the rest of the Encoder–Decoder model. This alignment model is illustrated on the righthand side of Figure 16-6.\n",
    "\n",
    "It starts with a time-distributed Dense layer with a single neuron, which receives as input all the encoder outputs, concatenated with the decoder's previous hidden state (e.g., $h_{(2)}$). This layer outputs a score (or energy) for each encoder output (e.g., $e_{(t, i)}$): this score measures how well each output is aligned\n",
    "with the decoder's previous hidden state. Finally, all the scores go through a softmax layer to get a final weight for each encoder output (e.g., $\\alpha_{(3, 2)}$). All the weights for a given decoder time step add up to 1 (since the softmax layer is not time-distributed). \n",
    "\n",
    "This particular attention mechanism is called **additive attention**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a603fd",
   "metadata": {},
   "source": [
    "There is also another mechanisms called **multiplicative attention**. Because the goal of the attention\n",
    "mechanism is to measure the similarity between one of the encoder’s outputs and\n",
    "the decoder’s previous hidden state, this other method proposes to simply compute the\n",
    "dot product (see Chapter 4) of these two vectors, as this is often a fairly good\n",
    "similarity measure, and modern hardware can compute it much faster. For this to\n",
    "be possible, both vectors must have the same dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0801fe",
   "metadata": {},
   "source": [
    "### Visual Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486cf6e",
   "metadata": {},
   "source": [
    "#### The Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada9062",
   "metadata": {},
   "source": [
    "In 2017, a team of Google managed to create an architecture called the **Transformer**, which significantly improved the state of the art in NMT without using any recurrent or convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14361096",
   "metadata": {},
   "source": [
    "![transformer_architecture](./images/ch16_transformer_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f0ff78",
   "metadata": {},
   "source": [
    "The details about the components of the transformer will not be covered here. Everything is in the book. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea48bab",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc1a20",
   "metadata": {},
   "source": [
    "1. **What are the pros and cons of using a stateful RNN versus a stateless RNN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715699f5",
   "metadata": {},
   "source": [
    "The advantage of using stateful RNNs is that it can capture patterns on sequences greater than the size of the windows the RNN is trained on. However, the preparation of the dataset is much harder, and better performance is not guaranteed since the data is not IID anymore, and that's important for SGD to succeed.\n",
    "\n",
    "On the other hand, training on stateless RNN is much simpler and data set preparation is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4bd773",
   "metadata": {},
   "source": [
    "2. **Why do people use Encoder–Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87fbec2",
   "metadata": {},
   "source": [
    "Encoder-Decoder RNNs \"reads\" the whole sentence first before translating it. Consequently, the result is much better. On the flip side, seq-to-seq RNNs translates the sequences as the words appears, this does not work well for most languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05cd0a",
   "metadata": {},
   "source": [
    "3. **How can you deal with variable-length input sequences? What about variable-length output sequences?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b854a8",
   "metadata": {},
   "source": [
    "Variable length input sequences can be handled using padding to ensure that all sequences have the same size, and also using masks to ensure that the model will ignore padding tokens. On the other hand, if you already know the size of the output, you can configure the loss function so that it ignores tokens that come after the end of the sequence. But if you don't know the output length, you can signal it training the model so that it outputs an end-of-sequence token at the end of each sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe408167",
   "metadata": {},
   "source": [
    "4. **What is beam search and why would you use it? What tool can you use to implement it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ac6bf",
   "metadata": {},
   "source": [
    "Beam search is a technique used to improve the performance of a\n",
    "trained Encoder–Decoder model, for example in a neural machine\n",
    "translation system. The algorithm keeps track of a short list of the k\n",
    "most promising output sentences (say, the top three), and at each\n",
    "decoder step it tries to extend them by one word; then it keeps only the\n",
    "k most likely sentences. The parameter k is called the beam width: the\n",
    "larger it is, the more CPU and RAM will be used, but also the more\n",
    "accurate the system will be. Instead of greedily choosing the most likely\n",
    "next word at each step to extend a single sentence, this technique allows\n",
    "the system to explore several promising sentences simultaneously.\n",
    "Moreover, this technique lends itself well to parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03785cc",
   "metadata": {},
   "source": [
    "5. **What is an attention mechanism? How does it help?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098aa623",
   "metadata": {},
   "source": [
    "An attention mechanism is a technique initially used in Encoder–\n",
    "Decoder models to give the decoder more direct access to the input\n",
    "sequence, allowing it to deal with longer input sequences. At each\n",
    "decoder time step, the current decoder’s state and the full output of the\n",
    "encoder are processed by an alignment model that outputs an alignment\n",
    "score for each input time step. This score indicates which part of the\n",
    "input is most relevant to the current decoder time step. The weighted\n",
    "sum of the encoder output (weighted by their alignment score) is then\n",
    "fed to the decoder, which produces the next decoder state and the output\n",
    "for this time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30478696",
   "metadata": {},
   "source": [
    "6. **What is the most important layer in the Transformer architecture? What is its purpose?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8aca4",
   "metadata": {},
   "source": [
    "The most important layer in the Transformer architecture is the Multi-\n",
    "Head Attention layer  Its purpose is to\n",
    "allow the model to identify which words are most aligned with each\n",
    "other, and then improve each word’s representation using these\n",
    "contextual clues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb7e0b",
   "metadata": {},
   "source": [
    "7. **When would you need to use sampled softmax?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33314583",
   "metadata": {},
   "source": [
    "Sampled softmax is used when training a classification model when\n",
    "there are many classes (e.g., thousands). It computes an approximation\n",
    "of the cross-entropy loss based on the logit predicted by the model for\n",
    "the correct class, and the predicted logits for a sample of incorrect\n",
    "words. This speeds up training considerably compared to computing the\n",
    "softmax over all logits and then estimating the cross-entropy loss. After\n",
    "training, the model can be used normally, using the regular softmax\n",
    "function to compute all the class probabilities based on all the logits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
