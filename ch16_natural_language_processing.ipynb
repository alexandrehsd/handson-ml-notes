{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cd6ed3",
   "metadata": {},
   "source": [
    "# Chapter 16 - Natural Language Processing with RNNs and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377e557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 11:17:03.835937: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-24 11:17:03.835969: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd24dfd",
   "metadata": {},
   "source": [
    "## Generating Shakesperian Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165267f2",
   "metadata": {},
   "source": [
    "Let's build a Char-RNN, an RNN whose task is to predict the next character in a sentence. We'll train the RNN using some sentences from Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0977ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\" # shortcut url\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bfb6e",
   "metadata": {},
   "source": [
    "Next, we must encode every character as an integer. One option is to create a custom preprocessing layer, as we did in Chapter 13. But in this case, it will be simpler to use Keras’s `Tokenizer` class. First we need to fit a tokenizer to the text: it will find all the characters used in the text and map each of them to a different character ID, from 1 to the number of distinct characters (it does not start at 0, so we can use that value for masking, as we will see later in this chapter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d9952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079ed163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f424a378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f010e43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of distinct characters\n",
    "max_id = len(tokenizer.word_index)\n",
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbacbea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of characters\n",
    "dataset_size = tokenizer.document_count\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b462b",
   "metadata": {},
   "source": [
    "Let's encode the full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than from 1 to 39):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a879abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc2bf3",
   "metadata": {},
   "source": [
    "To split the dataset into a training set, a validation set, and a test set, we can't simply shuffle it, since we are dealing with sequential data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b14cb",
   "metadata": {},
   "source": [
    "### How to Split a Sequential Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a6951d",
   "metadata": {},
   "source": [
    "It is very important to avoid any overlap between the training set, the validation set, and the test set. For example, we can take the first 90% of the text for the training set, then the next 5% for the validation set, and the final 5% for the test set. It would also be a good idea to leave a gap between these sets to avoid the risk of a paragraph overlapping over two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90b3f80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 11:17:46.311854: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-24 11:17:46.311888: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-24 11:17:46.311921: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (alexandre-Inspiron-5458): /proc/driver/nvidia/version does not exist\n",
      "2022-05-24 11:17:46.530533: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5803cafd",
   "metadata": {},
   "source": [
    "### Chopping the Sequential Dataset Into Multiple Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93d2dd",
   "metadata": {},
   "source": [
    "The training set now consists of a single sequence of over a million characters, so we can’t just train the neural network directly on it: the RNN would be equivalent to a deep net with over a million layers, and we would have a single (very long) instance to train it. Instead, we will use the dataset's `window()` method to convert this long sequence of characters into many smaller windows of text. Every instance in the dataset will be a fairly short substring of the whole text, and the RNN will be unrolled only over the length of these substrings. This is called *truncated backpropagation through time*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a37a4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1  # target = input shifted 1 character ahead\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ad8b9",
   "metadata": {},
   "source": [
    "The `window()` function returns a `nested dataset`, analogous to a list of lists. However, the `fit` method only accepts tensors as input. Therefore, we must use the function method `flat_map` to flatten that list of datasets. Note that we can also pass a function to `flat_map` to perform transform operations before flattening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "140e2701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since all windows have exactly that length, we will get a single tensor for each of them\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e678946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af880e23",
   "metadata": {},
   "source": [
    "In summary, so far we have performed the following operations:\n",
    "\n",
    "![pipeline_data_transformation](./images/ch16_pipeline_sequential_data_transformation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfb6a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the characters\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "# prefetching\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06177d",
   "metadata": {},
   "source": [
    "### Building and Training the Char-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0326d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, \n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7f730",
   "metadata": {},
   "source": [
    "### Using the Char-RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540f7d3",
   "metadata": {},
   "source": [
    "To feed new data to the model, we need to preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2fb7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6922ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]  # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d70c3",
   "metadata": {},
   "source": [
    "### Generating Fake Shakesperian Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433d938d",
   "metadata": {},
   "source": [
    "To generate new text using the Char-RNN model, we could feed it some text,\n",
    "make the model predict the most likely next letter, add it at the end of the text,\n",
    "then give the extended text to the model to guess the next letter, and so on. But\n",
    "in practice this often leads to the same words being repeated over and over\n",
    "again. Instead, we can pick the next character randomly, with a probability equal\n",
    "to the estimated probability, using TensorFlow’s `tf.random.categorical()`\n",
    "function. This will generate more diverse and interesting text. The\n",
    "`categorical()` function samples random class indices, given the class log\n",
    "probabilities (logits). To have more control over the diversity of the generated\n",
    "text, we can divide the logits by a number called the `temperature`, which we can\n",
    "tweak as we wish: a temperature close to 0 will favor the high-probability\n",
    "characters, while a very high temperature will give all characters an equal\n",
    "probability. The following `next_char()` function uses this approach to pick the\n",
    "next character to add to the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "841d654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess(text)\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944ffaeb",
   "metadata": {},
   "source": [
    "Next, we can write a small function that will repeatedly call `next_char()` to get\n",
    "the next character and append it to the given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2a7b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa2d2ab",
   "metadata": {},
   "source": [
    "Let's text some inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96b612cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c72d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(complete_text(\"w\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "606cf704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(complete_text(\"w\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2042d58c",
   "metadata": {},
   "source": [
    "### Stateful RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc6b21",
   "metadata": {},
   "source": [
    "Until now, we have used only *stateless* RNNs: at each training iteration the\n",
    "model starts with a hidden state full of zeros, then it updates this state at each\n",
    "time step, and after the last time step, it throws it away, as it is not needed\n",
    "anymore. What if we told the RNN to preserve this final state after processing\n",
    "one training batch and use it as the initial state for the next training batch? This\n",
    "way the model can learn long-term patterns despite only backpropagating\n",
    "through short sequences. This is called a *stateful* RNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b300f209",
   "metadata": {},
   "source": [
    "First, note that a stateful RNN only makes sense if each input sequence in a\n",
    "batch starts exactly where the corresponding sequence in the previous batch left\n",
    "off. So the first thing we need to do to build a stateful RNN is to use sequential\n",
    "and nonoverlapping input sequences (rather than the shuffled and overlapping\n",
    "sequences we used to train stateless RNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6546dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186641ba",
   "metadata": {},
   "source": [
    "![pipeline_stateful_rnn](./images/ch16_pipeline_stateful_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a2d7dd",
   "metadata": {},
   "source": [
    "Now let's create the stateful RNN:\n",
    "\n",
    "- Set `stateful=True`\n",
    "- The RNN needs to know the batch size (since it will preserve a state for each input sequence in the batch), so we must set `batch_input_shape` in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "758601c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True, \n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74642a1",
   "metadata": {},
   "source": [
    "We need to reset the states at the end of each epoch, to do this, we can use the following callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecde7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da295408",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d3f8b",
   "metadata": {},
   "source": [
    "After this model is trained, it will only be possible to use it to make predictions for batches of\n",
    "the same size as were used during training. To avoid this restriction, create an identical\n",
    "stateless model, and copy the stateful model’s weights to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40d27b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tdf3zwvcik :ma!&q. :phgr&;ubltcpzhp:'rv:cq3z!$ pau:\n"
     ]
    }
   ],
   "source": [
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "# Build the model\n",
    "stateless_model.build(tf.TensorShape([None, None, max_id]))\n",
    "stateless_model.set_weights(model.get_weights())\n",
    "model = stateless_model\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0035442",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7990bfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 7s 0us/step\n",
      "17473536/17464789 [==============================] - 7s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n",
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82f187",
   "metadata": {},
   "source": [
    "The reviews have already been preprocessed for us: each of which is\n",
    "represented as a NumPy array of integers, where each integer represents a word.\n",
    "All punctuation was removed, and then words were converted to lowercase, split\n",
    "by spaces, and finally indexed by frequency (so low integers correspond to\n",
    "frequent words). The integers 0, 1, and 2 are special: they represent the padding\n",
    "token, the start-of-sequence (SSS) token, and unknown words, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a4baef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 1s 1us/step\n",
      "1654784/1641221 [==============================] - 1s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode a review\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be669525",
   "metadata": {},
   "source": [
    "If we want to deploy the model to a mobile device or a web application, we may have to handle preprocessing only using TensorFlow operations, so it can be included in the model itself. Let's see how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fd5b535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the original IMDb dataset \n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "113545b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'unsupervised'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eab3ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = info.splits[\"train\"].num_examples\n",
    "test_size = info.splits[\"test\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66d243be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a64895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1daaf3a",
   "metadata": {},
   "source": [
    "Next, we need to construct the vocabulary. This requires going through the\n",
    "whole training set once, applying our `preprocess()` function, and using a\n",
    "Counter to count the number of occurrences of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae902f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8822815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e0c97da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53893"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b5fce",
   "metadata": {},
   "source": [
    "We probably don't need out model to know all the words in the dictionary, so let's keep only the 10000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0de3fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15951d78",
   "metadata": {},
   "source": [
    "Now we need to replace each word with its ID (i.e., its index in the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e33fed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd9179b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7343e3",
   "metadata": {},
   "source": [
    "Now we are ready to create the final training set. We batch the reviews, then\n",
    "convert them to short sequences of words using the `preprocess()` function,\n",
    "then encode these words using a `simple encode_words()` function that uses the\n",
    "table we just built, and finally prefetch the next batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "61175624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb971c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 128s 159ms/step - loss: 0.6151 - accuracy: 0.6264\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 102s 131ms/step - loss: 0.3842 - accuracy: 0.8332\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 113s 144ms/step - loss: 0.2245 - accuracy: 0.9171\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 109s 140ms/step - loss: 0.1448 - accuracy: 0.9493\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 123s 158ms/step - loss: 0.1215 - accuracy: 0.9579\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a5a45",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0a2c0",
   "metadata": {},
   "source": [
    "As it stands, the model will need to learn that the padding tokens should be\n",
    "ignored. But we already know that! Why don’t we tell the model to ignore the\n",
    "padding tokens, so that it can focus on the data that actually matters? It’s actually\n",
    "trivial: simply add `mask_zero=True` when creating the Embedding layer. This\n",
    "means that padding tokens (whose ID is 0) will be ignored by all downstream\n",
    "layers. That’s all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "877827d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 130s 156ms/step - loss: 0.5389 - accuracy: 0.7154\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 123s 157ms/step - loss: 0.3348 - accuracy: 0.8600\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 129s 165ms/step - loss: 0.1849 - accuracy: 0.9337\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 133s 170ms/step - loss: 0.1298 - accuracy: 0.9551\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 125s 160ms/step - loss: 0.1178 - accuracy: 0.9560\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True, # not shown in the book\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d475c",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1fee8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 12:22:26.846339: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 192762400 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86543504",
   "metadata": {},
   "source": [
    "The `hub.KerasLayer` layer downloads the module from the given URL. This\n",
    "particular module is a sentence encoder: it takes strings as input and encodes\n",
    "each one as a single vector (in this case, a 50-dimensional vector). Internally, it\n",
    "parses the string (splitting words on spaces) and embeds each word using an\n",
    "embedding matrix that was pretrained on a huge corpus: the Google News 7B\n",
    "corpus (seven billion words long!). Then it computes the mean of all the word\n",
    "embeddings, and the result is the sentence embedding. We can then add two\n",
    "simple Dense layers to create a good sentiment analysis model. By default, a\n",
    "hub.KerasLayer is not trainable, but you can set `trainable=True` when\n",
    "creating it to change that so that you can fine-tune it for your task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "950a85c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 6s 6ms/step - loss: 0.5473 - accuracy: 0.7266\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5143 - accuracy: 0.7484\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5092 - accuracy: 0.7510\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5058 - accuracy: 0.7538\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5031 - accuracy: 0.7548\n"
     ]
    }
   ],
   "source": [
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets[\"train\"].batch(batch_size).prefetch(1)\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efdc3ef",
   "metadata": {},
   "source": [
    "## An Encoder-Decoder Network for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29446a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
